{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025b6ecb-8581-47b1-9688-e7e70c17a7aa",
   "metadata": {},
   "source": [
    "# Word Embeddings and Connections\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "Today our learning goals are to be able to...\n",
    "\n",
    "1. Recognize and describe scenarios where word embeddings may be useful\n",
    "2. Use pretrained word embeddings to solve problems\n",
    "3. have intuitions about meaning encoded or not encoded within word vectors\n",
    "\n",
    "## Today's Activity\n",
    "\n",
    "Not only are word embeddings designed to be good encodings of word meaning, they are typically designed to encode *word similarity*. Luckily for us, identifying word similarity is a very common task in puzzle games, including the (hopefully somewhat recently) popular NYT game Connections. \n",
    "\n",
    "The task in this game is to partition 16 words into 4 groups of four which share some common property. This is typically a meaning-related property (though sometimes a particular instance of Connections involves spelling tricks, which our journey into tokenization tells us we may have trouble with). \n",
    "\n",
    "At the very least, this will be a reasonable test of the effectiveness of a vector embedding trained on co-occurance statistics!\n",
    "\n",
    "To begin, we'll load in our word-vectors of choice: 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) vectors. Like, skip-gram/word2vec, GloVe is an approach to training word embeddings based on word co-occurance. The primary difference is that GloVe vectors trained not on context prediction but to directly estimate *global* co-occurance probabilities (i.e., the *Glo* in GloVe). GloVe models pretrained over 6 billions words with various embedding sizes are available freely, so let's load in the 50d embeddings from the provided file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b23b85a5-d8a7-45bd-a44b-0912a5a59833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = {}\n",
    "with open(\"embeddings/glove.6B.50d.txt\") as embeddings_f:\n",
    "    for line in embeddings_f:\n",
    "        word, *embed = line.split()\n",
    "        embeddings[word] = np.array([float(x) for x in embed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b1377-bc70-4b7e-ab1c-5a343ad3d783",
   "metadata": {},
   "source": [
    "Now let's set up our game of Connections. Luckily, the game only really consists of 16 words, each represented as a string. Thus our game state can be entirely represented by a list with initial length 16!\n",
    "\n",
    "Here's the daily game from 10/27/2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f81a52ce-af11-4be3-8def-ad124a95389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/27/24\n",
    "game_1 = [\"fresh\", \"prince\", \"bel\", \"air\", \n",
    "          \"quality\", \"bar\", \"cute\", \"mermaid\", \n",
    "          \"lux\", \"wise\", \"tramp\", \"mood\", \n",
    "          \"feeling\", \"smart\", \"mole\", \"rascals\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a5333-44c4-43b5-88e4-dc702a6f58bd",
   "metadata": {},
   "source": [
    "We can also verify that all of the words are in our vocab. The following snipped would print any out-of-vocab words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ae7259b-b5a4-4eb9-a14e-ac484a375536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in game_1:\n",
    "    if w not in embeddings:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bc4d1-a073-467e-82bf-3c9dfeb27596",
   "metadata": {},
   "source": [
    "Now onto strategy. \n",
    "\n",
    "We can compute the similarity between two word vectors by computing their *cosine similarity*. That is, we can compute\n",
    "$$ \\text{sim}(w, v) = \\frac{w \\cdot v}{\\lvert w \\rvert \\lvert v \\rvert} $$\n",
    "\n",
    "Our goal will be to iteratively choose the combination of 4 words that maximizes the similarity between all of the words. \n",
    "\n",
    "We've defined the similarity between two words in terms of vectors, but it's still unclear what the similarity score between four words should be. For now, we'll define that as the sum of the similarities between every pair of words in the set of 4! **Write the two functions below to compute scores for both pairs of words and 4-tuples!**\n",
    "\n",
    "In order to do that, a few functions might be handy to save you some time. First, the `itertools` library has a function called `combinations(x, k)` that gets all possible unique $k$-tuples in x (assuming order doesn't matter). `permutations` does the same, but considers order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1cf1f0c-73b1-4c09-9ca8-6545c73fb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 'B')\n",
      "('A', 'C')\n",
      "('A', 'D')\n",
      "('B', 'C')\n",
      "('B', 'D')\n",
      "('C', 'D')\n",
      "---\n",
      "('A', 'B')\n",
      "('A', 'C')\n",
      "('A', 'D')\n",
      "('B', 'A')\n",
      "('B', 'C')\n",
      "('B', 'D')\n",
      "('C', 'A')\n",
      "('C', 'B')\n",
      "('C', 'D')\n",
      "('D', 'A')\n",
      "('D', 'B')\n",
      "('D', 'C')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations, permutations\n",
    "\n",
    "x = [\"A\", \"B\", \"C\", \"D\"]\n",
    "for y in combinations(x, 2):\n",
    "    print(y)\n",
    "\n",
    "print(\"---\")\n",
    "for y in permutations(x, 2):\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01930f-18d6-4dac-ab27-d67595c99eb6",
   "metadata": {},
   "source": [
    "`norm` from `numpy.linalg` also computes the norm (i.e., magnitude) of a vector for you. You can confirm it works in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a2a79d4-fba8-4a4e-9ae2-00b330c9a84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "v = np.array([1, 3, 7, 2, -1])\n",
    "\n",
    "# Use norm\n",
    "print(norm(v))\n",
    "\n",
    "# Compute norm \"by hand\"\n",
    "print(np.sqrt(sum(x**2 for x in v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8068f32-7fbe-4600-b1d2-1b057f5d5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Sequence\n",
    "\n",
    "def score_pair(embeddings : Mapping[str, Sequence[float]], w_1 : str, w_2 : str) -> float:\n",
    "    #TODO\n",
    "    return 0.0\n",
    "\n",
    "def score_row(embeddings : Mapping[str, Sequence[float]], row : Sequence[str]) -> float:\n",
    "    # TODO\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09014948-49b4-475a-8705-599f02627200",
   "metadata": {},
   "source": [
    "Now let's build a model to make a guess. We want to \n",
    "\n",
    "1. Score all possible combinations of 4 words that have not been selected (i.e., are in the parameter `remaining_words`)\n",
    "2. Find and return the top-`k` highest scoring combinations\n",
    "\n",
    "When guessing, we can proceed down the list until we guess right, at which point we can remove the guessed words from `remaining_words` and make a second guess!\n",
    "\n",
    "Write `make_guess`, which should accomplish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44261ab4-4a84-43b5-8fd0-2ff0caf87a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_guess(embeddings : Mapping[str, Sequence[float]], remaining_words : Sequence[str], k : int = 10) -> Sequence[:\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec45a97-00c3-4d4d-b229-c05cc239d82c",
   "metadata": {},
   "source": [
    "Now use the following box to play the game! What kinds of semantic relationships does it get right? What does it get wrong? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121cb75-d742-43a2-9095-defb75bae5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3800877c-03d0-42a3-9c32-26ebdb1b06c5",
   "metadata": {},
   "source": [
    "Now let's consider how well our model does with respect to the solution. Here are the true partitions, along with the NYT provided category labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf7aaf79-4fcb-4a4b-90e1-6031d35fb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [\n",
    "    [\"air\", \"feeling\", \"mood\", \"quality\"],   # Ambience\n",
    "    [\"cute\", \"fresh\", \"smart\", \"wise\"],      # Sassy\n",
    "    [\"bar\", \"bel\", \"lux\", \"mole\"],           # Units \n",
    "    [\"mermaid\", \"prince\", \"rascal\", \"tramp\"] # The little _\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570d69b-fe9c-40a8-aa6d-d297fa48dda2",
   "metadata": {},
   "source": [
    "For the purpose of evaluating the model, we can begin to ask how well the model can guess a correct partition. There are a couple of ways to measure this. For example, we can do what the NYT does: Count the number of guesses it would take to ge tthe correct answer. This is equivalent to getting the the minimum 0-indexed *rank* (i.e., position in the output list) of a correct grouping, removing those words and repeating. Our score would be the sum of all of those ranks! We win if the sum of ranks is less than the number of mistakes we're allowed to make (5 by the NYT's rules). \n",
    "\n",
    "Another approach is to take the *top-k* accuracy. That is, compute model accuracy, but the answer counts as correct if it's in the top-k provided options.\n",
    "\n",
    "One other (more sensitive) measure is the MRR (Mean Reciprocal Rank). Here, for each prompt, we take the *reciprocal* of the best answer's rank (i.e., $\\frac{1}{\\text{rank}}$). The MRR is, as the name implies, the mean of all of these reciprocals. This is also the reciprocal of the harmonic mean of the ranks!\n",
    "\n",
    "Of course, computing these over a single example is a bit lackluster. If you're not up to designing your own games to test, you can consider evaluating this model with [this Kaggle dataset](https://www.kaggle.com/datasets/eric27n/the-new-york-times-connections) of historical Connections games. The data is loaded in in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6777a9ec-e5eb-41a8-b77e-c561677d4361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "connections_rows = []\n",
    "with open(\"data/connections.csv\") as in_f:\n",
    "    data_reader = csv.DictReader(in_f)\n",
    "\n",
    "    for row in data_reader:\n",
    "        connections_rows.append(row)\n",
    "\n",
    "n_games = max([int(row[\"Game ID\"]) for row in connections_rows])\n",
    "n_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c15cb19d-b973-4a8b-a53e-d1127d29219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes rows are ordered so this is a bit less time consuming\n",
    "\n",
    "connections_rows = sorted(connections_rows, key=lambda x: int(x[\"Game ID\"]))\n",
    "\n",
    "eval_data = []\n",
    "current_game_id = 1\n",
    "current_game_rows = []\n",
    "for row in connections_rows:\n",
    "    if int(row[\"Game ID\"]) == current_game_id:\n",
    "        current_game_rows.append(row)\n",
    "\n",
    "    else:\n",
    "        # Insert the found game\n",
    "        board = [row[\"Word\"].lower() for row in current_game_rows]\n",
    "        solution = [[row[\"Word\"].lower() for row in current_game_rows \n",
    "                     if (int(row[\"Group Level\"]) == level)] \n",
    "                    for level in range(4)]\n",
    "        eval_data.append((board, solution))\n",
    "\n",
    "        # Reset for next iter\n",
    "        current_game_rows = [row]\n",
    "        current_game_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53a47ddb-69e4-416c-93f5-4a79394c6e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Filter games that have out-of-vocab words\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21min_vocab\u001b[39m(embeddings : \u001b[43mMapping\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Sequence[\u001b[38;5;28mfloat\u001b[39m]], board : Sequence[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m board:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embeddings:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Mapping' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter games that have out-of-vocab words\n",
    "def in_vocab(embeddings : Mapping[str, Sequence[float]], board : Sequence[str]):\n",
    "    for word in board:\n",
    "        if word not in embeddings:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filtered_eval_data = [(board, solution) for board, solution in eval_data if in_vocab(embeddings, board)]\n",
    "\n",
    "print(\"{} of {} in-vocab games to evaluate on\".format(len(filtered_eval_data), len(eval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac1af49c-12eb-4205-921d-cee2124abf3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(embeddings : \u001b[43mMapping\u001b[49m[\u001b[38;5;28mstr\u001b[39m, Sequence[\u001b[38;5;28mfloat\u001b[39m]], data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# TODO Compute some metric to measure our model's performance. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Mapping' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(embeddings : Mapping[str, Sequence[float]], data) -> float:\n",
    "    # TODO Compute some metric to measure our model's performance. \n",
    "    return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac80cc7c-2b29-4ac3-b61c-f870683a500b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m(embeddings, eval_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(embeddings, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c237c-f0b0-4514-9f9b-9026d458f62a",
   "metadata": {},
   "source": [
    "If you have additional time and want to get even more familiar with word embeddings and their quirks, consider [Semantle](https://semantle.com/), a word-game that directly evaluates your ability to intuit meaning as represented by word vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8519ddb-9e1b-4a93-b1d6-a4fc023e4234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
